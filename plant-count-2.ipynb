{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12097687,"sourceType":"datasetVersion","datasetId":7615976},{"sourceId":12238609,"sourceType":"datasetVersion","datasetId":7711235},{"sourceId":12238681,"sourceType":"datasetVersion","datasetId":7711287},{"sourceId":12308991,"sourceType":"datasetVersion","datasetId":7758516},{"sourceId":12317200,"sourceType":"datasetVersion","datasetId":7763803},{"sourceId":12317400,"sourceType":"datasetVersion","datasetId":7763952},{"sourceId":12319290,"sourceType":"datasetVersion","datasetId":7765149},{"sourceId":12319399,"sourceType":"datasetVersion","datasetId":7765221},{"sourceId":12364747,"sourceType":"datasetVersion","datasetId":7795960}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics roboflow opencv-python matplotlib pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom roboflow import Roboflow\nimport numpy as np\nfrom PIL import Image\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install roboflow\n\n\ndef download_dataset():\n    from roboflow import Roboflow\n    rf = Roboflow(api_key=\"QHr9qKdWagsf1TvMIUq1\")\n    project = rf.workspace(\"sanjrani\").project(\"potato-plant-30d\")\n    version = project.version(9)\n    dataset = version.download(\"yolov11\")\n    return dataset.location\n\n# Uncomment to download dataset\ndataset_path = download_dataset()\nprint(f\"Dataset downloaded to: {dataset_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2.5: Check GPU first (run this before training)\ndef check_gpu_setup():\n    import torch\n    if torch.cuda.is_available():\n        print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n        print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n        return True\n    else:\n        print(\"‚ùå GPU not available - check Kaggle accelerator settings\")\n        return False\n\ncheck_gpu_setup()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(dataset_path):\n    import torch\n    \n    # Explicitly check and set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"üöÄ Training on: {device}\")\n    \n    # Load model\n    model = YOLO('yolo11m-seg.pt')  # Medium model for better accuracy\n    \n    # Train with explicit GPU settings\n    results = model.train(\n        data=f\"{dataset_path}/data.yaml\",\n        epochs=10,  # Your original setting\n        imgsz=640,\n        batch=16,  # Increased for GPU\n        device=device,  # Explicitly set device\n        workers=2,  # Good for Kaggle\n        name='potato_counter'\n    )\n    \n    return \"runs/segment/potato_counter3/weights/best.pt\"\n\n# Your training call\nmodel_path = train_model(\"/kaggle/working/potato-plant-30D-9\")\nprint(f\"Model trained and saved to: {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your trained model (replace with your model path)\nMODEL_PATH = '/kaggle/input/orthofiles/best.pt'  # Update this path\nmodel = YOLO(MODEL_PATH)\nprint(\"Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:41:36.680980Z","iopub.execute_input":"2025-06-29T11:41:36.681252Z","iopub.status.idle":"2025-06-29T11:41:36.768387Z","shell.execute_reply.started":"2025-06-29T11:41:36.681231Z","shell.execute_reply":"2025-06-29T11:41:36.767669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt \ndef count_potato_plants(image_path, confidence=0.5, show_image=True):\n    \"\"\"\n    Count potato plants in an image\n    \n    Args:\n        image_path: Path to the image\n        confidence: Detection confidence threshold (0.0 - 1.0)\n        show_image: Whether to display the annotated image\n    \n    Returns:\n        int: Number of plants detected\n    \"\"\"\n    \n    # Read image\n    image = cv2.imread(IMAGE_PATH)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Run inference\n    results = model(image, conf=confidence)\n    \n    # Count detections\n    plant_count = 0\n    if results[0].boxes is not None:\n        plant_count = len(results[0].boxes)\n    \n    # Show annotated image\n    if show_image:\n        # Get annotated image from YOLO\n        annotated_image = results[0].plot()\n        \n        # Display using matplotlib\n        plt.figure(figsize=(10, 5))\n        plt.imshow(annotated_image)\n        plt.title(f'Detected {plant_count} Potato Plants', fontsize=16)\n        plt.axis('off')\n        plt.show()\n    \n    print(f\"üå± Found {plant_count} potato plants!\")\n    return plant_count\nIMAGE_PATH = \"/kaggle/input/tif-files/DJI_20250624143217_0002_D.tif\"\n\n# Count plants with default settings\nplant_count = count_potato_plants(IMAGE_PATH, confidence=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T11:41:39.981369Z","iopub.execute_input":"2025-06-29T11:41:39.981874Z","iopub.status.idle":"2025-06-29T11:41:42.411678Z","shell.execute_reply.started":"2025-06-29T11:41:39.981850Z","shell.execute_reply":"2025-06-29T11:41:42.411034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt \ndef count_potato_plants(image_path, confidence=0.5, show_image=True):\n    \"\"\"\n    Count potato plants in an image\n    \n    Args:\n        image_path: Path to the image\n        confidence: Detection confidence threshold (0.0 - 1.0)\n        show_image: Whether to display the annotated image\n    \n    Returns:\n        int: Number of plants detected\n    \"\"\"\n    \n    # Read image\n    image = cv2.imread(IMAGE_PATH)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Run inference\n    results = model(image, conf=confidence)\n    \n    # Count detections\n    plant_count = 0\n    if results[0].boxes is not None:\n        plant_count = len(results[0].boxes)\n    \n    # Show annotated image\n    if show_image:\n        # Get annotated image from YOLO\n        annotated_image = results[0].plot()\n        \n        # Display using matplotlib\n        plt.figure(figsize=(15, 10))\n        plt.imshow(annotated_image)\n        plt.title(f'Detected {plant_count} Potato Plants', fontsize=16)\n        plt.axis('off')\n        plt.show()\n    \n    print(f\"üå± Found {plant_count} potato plants!\")\n    return plant_count\nIMAGE_PATH = \"/kaggle/input/sampling-dataset/s3 blue hoop 4m.jpg\"\n\n# Count plants with default settings\nplant_count = count_potato_plants(IMAGE_PATH, confidence=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:45:25.611432Z","iopub.execute_input":"2025-06-21T14:45:25.611726Z","iopub.status.idle":"2025-06-21T14:45:28.665194Z","shell.execute_reply.started":"2025-06-21T14:45:25.611705Z","shell.execute_reply":"2025-06-21T14:45:28.664126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ultralytics sahi opencv-python-headless\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Increase pixel limit before any OpenCV import\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(10**13)  # 1 trillion pixels\n\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom sahi import AutoDetectionModel\n\nfrom sahi.predict import get_sliced_prediction\nimport cv2\n\n# Load image with PIL (handles large files better)\nimage_path = '/kaggle/input/potato-data/D0580B97-FCA7-4FDE-B4EF-57E709DB8CBE.jpeg'\npil_image = Image.open(image_path)\northo_image = np.array(pil_image)  # Shape: (height, width, 3)\n\n# Convert RGB to BGR for OpenCV compatibility\northo_image = cv2.cvtColor(ortho_image, cv2.COLOR_RGB2BGR)\n\n# Initialize model (corrected parameters)\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='yolov8',\n    model_path='/kaggle/input/orthofiles/best.pt',\n    confidence_threshold=0.2,\n    device='cuda:0' if torch.cuda.is_available() else 'cpu',\n    category_mapping={'0': 'potato'}  # Replace with your class mapping\n)\n\n# Perform tiled inference\nresult = get_sliced_prediction(\n    ortho_image,  # Use the numpy array directly\n    detection_model,\n    slice_height=3000,\n    slice_width=3000,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:10:00.687473Z","iopub.execute_input":"2025-06-29T12:10:00.688342Z","iopub.status.idle":"2025-06-29T12:11:03.422733Z","shell.execute_reply.started":"2025-06-29T12:10:00.688309Z","shell.execute_reply":"2025-06-29T12:11:03.422084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result.export_visuals('output_dir2/')  # Saves mask overlays\n# result.to_coco('annotations.json')     # COCO format for GIS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:11:03.424204Z","iopub.execute_input":"2025-06-29T12:11:03.424811Z","execution_failed":"2025-06-29T12:15:34.713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Display composite visualization\ndisplay(Image(filename='/kaggle/working/output_dir2/prediction_visual.png'))\n\n# Display sample mask\n# display(Image(filename='output/instance_0_mask.png'))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-29T12:15:34.714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Set environment variables FIRST\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(10**13)  # 1 trillion pixels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:06:15.829143Z","iopub.execute_input":"2025-06-29T12:06:15.829393Z","iopub.status.idle":"2025-06-29T12:06:15.833475Z","shell.execute_reply.started":"2025-06-29T12:06:15.829376Z","shell.execute_reply":"2025-06-29T12:06:15.832792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install missing dependencies\n!pip install -q torch sahi opencv-python-headless matplotlib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:42:14.836696Z","iopub.execute_input":"2025-07-03T17:42:14.837234Z","iopub.status.idle":"2025-07-03T17:42:18.449879Z","shell.execute_reply.started":"2025-07-03T17:42:14.837205Z","shell.execute_reply":"2025-07-03T17:42:18.448330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:42:18.933226Z","iopub.execute_input":"2025-07-03T17:42:18.933982Z","iopub.status.idle":"2025-07-03T17:42:22.564502Z","shell.execute_reply.started":"2025-07-03T17:42:18.933942Z","shell.execute_reply":"2025-07-03T17:42:22.563410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Set environment variables FIRST\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(10**13)  # 1 trillion pixels\n\n\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Load and preprocess image\nimage_path = '/kaggle/input/sampling-dataset/s3 yellow hoop 4m.jpg'\npil_image = Image.open(image_path)\northo_image = np.array(pil_image)\northo_image = cv2.cvtColor(ortho_image, cv2.COLOR_RGB2BGR)\n\n# Initialize model\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='yolov8',\n    model_path='/kaggle/input/potatot30d-weights/potato t30D.pt',\n    confidence_threshold=0.1,\n    device='cuda:0' if torch.cuda.is_available() else 'cpu',\n    category_mapping={'0': 'potato'}\n)\n\n# Perform inference\nresult = get_sliced_prediction(\n    ortho_image,\n    detection_model,\n    slice_height=3000,\n    slice_width=3000,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2\n)\n\n# Count plants\nplant_count = len(result.object_prediction_list)\nprint(f\"Detected plants: {plant_count}\")\n\nresult.export_visuals('output_dir2/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:23:43.234739Z","iopub.execute_input":"2025-07-03T18:23:43.235320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Display composite visualization\ndisplay(Image(filename='/kaggle/working/output_dir2/prediction_visual.png'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:25:37.251899Z","iopub.execute_input":"2025-07-03T18:25:37.252623Z","iopub.status.idle":"2025-07-03T18:25:37.986213Z","shell.execute_reply.started":"2025-07-03T18:25:37.252596Z","shell.execute_reply":"2025-07-03T18:25:37.984692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_potato_plants(image_path, confidence, show_image=False):\n    import os\n    import cv2\n    print(f\"\\nüî• Processing NEW image: {image_path}\")\n    img = cv2.imread(image_path)\n    print(f\"Image shape: {img.shape} | Mean pixel value: {img.mean():.1f}\")\n    \n\n    \n    results = {}\n    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n    \n    # Get all image files (with absolute paths)\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend([\n            os.path.join(image_folder, f) \n            for f in os.listdir(image_folder) \n            if f.lower().endswith(ext)\n        ])\n    \n    print(f\"Processing {len(image_files)} images...\")\n    \n    total_plants = 0\n    for i, image_path in enumerate(image_files, 1):\n        try:\n            # Verify the image is readable and unique\n            img = cv2.imread(image_path)\n            if img is None:\n                print(f\"‚ö†Ô∏è Could not read: {image_path}\")\n                results[os.path.basename(image_path)] = 0\n                continue\n                \n            # DEBUG: Print image hash to verify uniqueness\n            img_hash = hash(img.tobytes())\n            print(f\"Processing {i}/{len(image_files)}: {os.path.basename(image_path)} [Hash: {img_hash}]\")\n            \n            # Count plants\n            count = count_potato_plants(image_path, confidence, show_image=False)\n            \n            results[os.path.basename(image_path)] = count\n            total_plants += count\n            \n            # Force cleanup\n            del img\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error processing {os.path.basename(image_path)}: {str(e)}\")\n            results[os.path.basename(image_path)] = 0\n    \n    print(f\"\\nüìä SUMMARY:\")\n    print(f\"Total Images: {len(image_files)}\")\n    print(f\"Total Plants: {total_plants}\")\n    print(f\"Average Plants per Image: {total_plants/len(image_files):.1f}\")\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}